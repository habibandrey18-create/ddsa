# services/smart_search_service.py - –£–º–Ω—ã–π –∞–≤—Ç–æ–ø–æ–∏—Å–∫ —Å offset per keyword
"""
Smart search service for Yandex Market.
Combines search engine, product processing, and data extraction functionality.
"""
import asyncio
import json
import os
import logging
from typing import List, Dict, Any, Optional, Tuple

import src.config as config
from src.core.database import get_postgres_db
from src.core.redis_cache import get_redis_cache

# Import mixins
from .search.search_engine import SearchEngineMixin
from .search.product_processor import ProductProcessorMixin

# Playwright for anti-bot bypass (fallback only)
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    async_playwright = None

logger = logging.getLogger(__name__)


class SmartSearchService(SearchEngineMixin, ProductProcessorMixin):
    """–£–º–Ω—ã–π –∞–≤—Ç–æ–ø–æ–∏—Å–∫ —Å offset per keyword"""

    def __init__(self):
        self.db = get_postgres_db() if config.USE_POSTGRES else None
        self.redis = get_redis_cache() if config.USE_REDIS else None
        self._session = None
        self._last_catalog_parse = self._load_parse_cache()  # –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π –∫—ç—à
        self._semaphore = asyncio.Semaphore(3)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ concurrency –¥–æ 3
        self._playwright_daily_count = 0  # –°—á–µ—Ç—á–∏–∫ Playwright –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π –∑–∞ –¥–µ–Ω—å
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.metrics = {
            'catalog_requests': 0,
            'catalog_errors': 0,
            'products_parsed': 0,
            'playwright_fallback_used': 0,
            'shadow_ban_detected': 0
        }

    def _load_parse_cache(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –≤—Ä–µ–º–µ–Ω–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        cache_file = "catalog_parse_cache.json"
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load parse cache: {e}")
        return {}

    def _save_parse_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –≤—Ä–µ–º–µ–Ω–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —Ñ–∞–π–ª"""
        cache_file = "catalog_parse_cache.json"
        try:
            with open(cache_file, 'w') as f:
                json.dump(self._last_catalog_parse, f, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save parse cache: {e}")

    def _can_use_playwright_fallback(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ –Ω–∞ Playwright fallback (max 5 per day)"""
        return self._playwright_daily_count < 5

    def get_metrics(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        return {
            **self.metrics,
            'playwright_daily_count': self._playwright_daily_count,
            'cache_size': len(self._last_catalog_parse)
        }

    async def get_session(self):
        """–ü–æ–ª—É—á–∏—Ç—å HTTP —Å–µ—Å—Å–∏—é"""
        if self._session is None:
            import aiohttp
            timeout = aiohttp.ClientTimeout(total=30)
            self._session = aiohttp.ClientSession(timeout=timeout)
        return self._session

    async def close_session(self):
        """–ó–∞–∫—Ä—ã—Ç—å HTTP —Å–µ—Å—Å–∏—é"""
        if self._session:
            await self._session.close()
            self._session = None

    async def crawl_catalogs(self, max_catalogs: int = 5) -> Tuple[int, int]:
        """
        –û–±—Ö–æ–¥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–∞

        Args:
            max_catalogs: –ú–∞–∫—Å–∏–º—É–º –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

        Returns:
            Tuple[int, int]: (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ_—Ç–æ–≤–∞—Ä–æ–≤, –ø—Ä–æ–ø—É—â–µ–Ω–æ_—Ç–æ–≤–∞—Ä–æ–≤)
        """
        logger.info(f"Starting catalog crawl with max {max_catalogs} catalogs")

        total_added = 0
        total_skipped = 0

        try:
            # –°–ø–∏—Å–æ–∫ URL –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞
            catalog_urls = [
                "https://market.yandex.ru/catalog--naushniki/",
                "https://market.yandex.ru/catalog--smartfony/",
                "https://market.yandex.ru/catalog--noutbuki/",
                "https://market.yandex.ru/catalog--planshety/",
                "https://market.yandex.ru/catalog--smart-chasy/",
            ][:max_catalogs]

            for url in catalog_urls:
                try:
                    async with self._semaphore:
                        added, skipped = await self._process_catalog(url)
                        total_added += added
                        total_skipped += skipped

                        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∫–∞—Ç–∞–ª–æ–≥–∞–º–∏
                        await asyncio.sleep(2)

                except Exception as e:
                    logger.error(f"Failed to process catalog {url}: {e}")
                    self.metrics['catalog_errors'] += 1

            logger.info(f"Catalog crawl completed: {total_added} added, {total_skipped} skipped")

        except Exception as e:
            logger.error(f"Catalog crawl failed: {e}")

        return total_added, total_skipped

    async def _process_catalog(self, url: str) -> Tuple[int, int]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω –∫–∞—Ç–∞–ª–æ–≥"""
        logger.info(f"Processing catalog: {url}")

        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML –∫–∞—Ç–∞–ª–æ–≥–∞
            html = await self._fetch_catalog_page(url)
            if not html:
                return 0, 0

            # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã –∏–∑ HTML
            products = self._parse_catalog_products(html, url)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
            added, skipped = await self._process_found_products(products, url)

            logger.info(f"Catalog {url} processed: {len(products)} found, {added} added, {skipped} skipped")
            return added, skipped

        except Exception as e:
            logger.error(f"Failed to process catalog {url}: {e}")
            return 0, 0

    async def _fetch_catalog_page(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
        try:
            session = await self.get_session()
            from src.utils.scraper import fetch_with_backoff
            html = await fetch_with_backoff(url, session, max_attempts=3)

            if html and len(html.strip()) > 1000:
                self.metrics['catalog_requests'] += 1
                return html
            else:
                logger.warning(f"Invalid HTML received for catalog {url}")
                return None

        except Exception as e:
            logger.error(f"Failed to fetch catalog page {url}: {e}")
            self.metrics['catalog_errors'] += 1
            return None

    def _parse_catalog_products(self, html: str, url: str = "") -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ HTML –∫–∞—Ç–∞–ª–æ–≥–∞
        –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ __NEXT_DATA__, –∑–∞—Ç–µ–º fallback –Ω–∞ HTML –ø–∞—Ä—Å–∏–Ω–≥
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ __NEXT_DATA__
            products = self._extract_items_from_next_data(html)
            if products:
                logger.info(f"Extracted {len(products)} products from __NEXT_DATA__")
                return products

            # Fallback: HTML –ø–∞—Ä—Å–∏–Ω–≥
            products = self._parse_catalog_fallback(html)
            logger.info(f"Extracted {len(products)} products via HTML fallback")
            return products

        except Exception as e:
            logger.error(f"Failed to parse catalog products: {e}")
            return []

    def _extract_items_from_next_data(self, html: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ __NEXT_DATA__"""
        try:
            import re
            next_data_match = re.search(r'__NEXT_DATA__\s*=\s*({.+?});', html, re.DOTALL)
            if not next_data_match:
                return []

            next_data = json.loads(next_data_match.group(1))
            return self._parse_next_data_products(next_data)

        except Exception as e:
            logger.debug(f"Failed to extract from __NEXT_DATA__: {e}")
            return []

    def _parse_next_data_products(self, data: Dict) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ NEXT_DATA —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        products = []

        try:
            # –ò—â–µ–º —Ç–æ–≤–∞—Ä—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if 'props' in data and 'pageProps' in data['props']:
                page_props = data['props']['pageProps']

                # –ò—â–µ–º –≤ catalogData
                if 'catalogData' in page_props:
                    catalog_data = page_props['catalogData']
                    if 'products' in catalog_data:
                        for item in catalog_data['products']:
                            product = self._convert_item_to_product(item)
                            if product:
                                products.append(product)

        except Exception as e:
            logger.debug(f"Failed to parse NEXT_DATA products: {e}")

        return products

    def _convert_item_to_product(self, item: Dict) -> Optional[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç NEXT_DATA –≤ —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–¥—É–∫—Ç–∞"""
        try:
            product = {
                'id': str(item.get('id', '')),
                'title': item.get('title', ''),
                'price': item.get('price', {}).get('value'),
                'url': item.get('link', {}).get('href', ''),
                'vendor': item.get('vendor', {}).get('name', ''),
                'rating': item.get('rating', 0),
                'reviews_count': item.get('reviewsCount', 0),
                'image_url': item.get('image', {}).get('url'),
                'has_images': bool(item.get('image')),
                'category': item.get('category', ''),
            }

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if product['id'] and product['title'] and product['url']:
                return product

        except Exception as e:
            logger.debug(f"Failed to convert item to product: {e}")

            return None

    def _parse_catalog_fallback(self, html: str) -> List[Dict]:
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ HTML"""
        products = []

        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')

            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Ç–æ–≤–∞—Ä–æ–≤
            product_selectors = [
                '[data-zone="product"]',
                '.product-card',
                '.catalog-product',
                '[data-product-id]'
            ]

            for selector in product_selectors:
                cards = soup.select(selector)
                for card in cards:
                    product = self._parse_product_card(card)
                    if product:
                        products.append(product)

                if products:
                    break

        except Exception as e:
            logger.error(f"Catalog fallback parsing failed: {e}")

        return products

    def _parse_product_card(self, card) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞
            product_id = card.get('data-product-id') or card.get('data-id')
            title_elem = card.select_one('[data-zone="title"], .title, h3, h4')
            price_elem = card.select_one('[data-zone="price"], .price')

            if not title_elem or not product_id:
                return None

            title = title_elem.get_text(strip=True)
            price_text = price_elem.get_text(strip=True) if price_elem else None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
            price = None
            if price_text:
                import re
                price_match = re.search(r'(\d+(?:\s*\d+)*(?:[.,]\d{1,2})?)', price_text.replace(' ', ''))
                if price_match:
                    price = float(price_match.group(1).replace(',', '.'))

            return {
                'id': str(product_id),
                'title': title,
                'price': price,
                'url': f"https://market.yandex.ru/product/{product_id}",
                'has_images': True,
            }

        except Exception as e:
            logger.debug(f"Failed to parse product card: {e}")
            return None

    async def _process_found_products(self, products: List[Dict], source_url: str) -> Tuple[int, int]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã"""
        added = 0
        skipped = 0

        for product in products:
            try:
                await self._enqueue_product(product, source_url)
                added += 1
                self.metrics['products_parsed'] += 1

            except Exception as e:
                logger.error(f"Failed to process product {product.get('title', 'Unknown')}: {e}")
                skipped += 1

        return added, skipped

    async def run_smart_search_cycle(self, max_catalogs: int = 5):
        """
        –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        Args:
            max_catalogs: –ú–∞–∫—Å–∏–º—É–º –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info("üöÄ Starting Advanced Yandex.Market Bot Worker")

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            logger.info("üîç Checking database connections...")
            if self.db:
                logger.info("‚úÖ Postgres connection OK")
            if self.redis:
                logger.info("‚úÖ Redis connection OK")
            logger.info("‚úÖ All database connections verified")

            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã
            logger.info("üîÑ Starting background services...")
            # Publish service is handled separately
            logger.info("‚úÖ Background services started")

            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ–∏—Å–∫–∞
            logger.info("üîÑ Starting main work loop")

            while True:
                try:
                    # –û–±—Ö–æ–¥–∏–º –∫–∞—Ç–∞–ª–æ–≥–∏
                    await self.crawl_catalogs(max_catalogs)

                    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    await self._run_keyword_searches()

                    # –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
                    await self._run_maintenance()

                    # –ñ–¥–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞
                    await asyncio.sleep(300)  # 5 –º–∏–Ω—É—Ç

        except Exception as e:
                    logger.error(f"Work cycle error: {e}")
                    await asyncio.sleep(60)

        except Exception as e:
            logger.error(f"Smart search cycle failed: {e}")
        finally:
            await self.close_session()

    async def _run_keyword_searches(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        try:
            keywords = getattr(config, 'AUTO_SEARCH_QUERIES', '').split(',')
            if not keywords or keywords == ['']:
                return

            for keyword in keywords[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                keyword = keyword.strip()
                if keyword:
                    await self._run_smart_search(keyword, max_pages=2)

        except Exception as e:
            logger.error(f"Keyword search failed: {e}")

    async def _run_maintenance(self):
        """–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à
            self._save_parse_cache()

            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∑–∞–¥–∞—á–∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è

        except Exception as e:
            logger.error(f"Maintenance failed: {e}")

    async def reset_search_state(self, key_text: str = None):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
        try:
            if self.db:
            if key_text:
                    self.db.reset_search_key(key_text)
            else:
                    self.db.reset_all_search_keys()
            logger.info(f"Search state reset for {key_text or 'all keys'}")
        except Exception as e:
            logger.error(f"Failed to reset search state: {e}")

    def get_search_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞"""
        try:
            stats = {
                'metrics': self.get_metrics(),
                'cache_info': {
                    'size': len(self._last_catalog_parse),
                    'last_updated': max(self._last_catalog_parse.values()) if self._last_catalog_parse else None
                }
            }

            if self.db:
                stats['db_stats'] = self.db.get_search_stats()

            return stats

        except Exception as e:
            logger.error(f"Failed to get search stats: {e}")
            return {}


# Backward compatibility
class SimpleSmartSearch(SmartSearchService):
    """–ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º offsets"""

    def __init__(self):
        super().__init__()
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–º–µ—â–µ–Ω–∏—è (offset) –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if os.path.exists(getattr(config, 'OFFSET_FILE', 'search_offsets.json')):
            try:
                with open(getattr(config, 'OFFSET_FILE', 'search_offsets.json'), "r", encoding="utf-8") as f:
                    self.offsets = json.load(f)
            except Exception:
                self.offsets = {}
        else:
            self.offsets = {}

    def search(self, keywords: list):
        """
        –ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–æ–∏—Å–∫ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞.
        """
        results = []
        for kw in keywords:
            offset = self.offsets.get(kw, 0)
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ (–∑–∞–≥–ª—É—à–∫–∞)
            new_results = []  # –ù–∞–ø—Ä–∏–º–µ—Ä: market_api.search(kw, offset)
            results.extend(new_results)
            self.offsets[kw] = offset + len(new_results)

        offset_file = getattr(config, 'OFFSET_FILE', 'search_offsets.json')
        with open(offset_file, "w", encoding="utf-8") as f:
            json.dump(self.offsets, f)


# Factory function for backward compatibility
def get_smart_search_service():
    """Get smart search service instance"""
    return SmartSearchService()