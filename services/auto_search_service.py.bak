# services/auto_search_service.py
"""–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–µ"""
import asyncio
import logging
import re
import os
import json
import random
from typing import List, Dict, Optional
from urllib.parse import urlparse, urlencode, quote
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π HTTP –∫–ª–∏–µ–Ω—Ç
from services.http_client import HTTPClient
from services.metrics import Metrics, Timer
from services.filter_service import should_filter_product
from database import make_product_key
from datetime import datetime

# ENHANCEMENT: Real User-Agent rotation to avoid detection
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# ENHANCEMENT: Expanded category keywords for deeper search
SEARCH_KEYWORDS = [
    # Electronics
    "–Ω–∞—É—à–Ω–∏–∫–∏",
    "—Å–º–∞—Ä—Ç—Ñ–æ–Ω",
    "–ø–ª–∞–Ω—à–µ—Ç",
    "–Ω–æ—É—Ç–±—É–∫",
    "—Ç–µ–ª–µ—Ñ–æ–Ω",
    "—á–∞—Å—ã",
    "—Ñ–∏—Ç–Ω–µ—Å-–±—Ä–∞—Å–ª–µ—Ç",
    # Home & Garden
    "–ø–æ—Å—É–¥–∞",
    "—Ç–µ–∫—Å—Ç–∏–ª—å",
    "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
    "—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫",
    "–º–µ–±–µ–ª—å",
    # Beauty & Health
    "–∫–æ—Å–º–µ—Ç–∏–∫–∞",
    "–ø–∞—Ä—Ñ—é–º",
    "—à–∞–º–ø—É–Ω—å",
    "–∫—Ä–µ–º",
    "–º–∞—Å–∫–∞",
    # Kids
    "–∏–≥—Ä—É—à–∫–∏",
    "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä",
    "–ø–∞–∑–ª—ã",
    "–∫–Ω–∏–≥–∞ –¥–µ—Ç—Å–∫–∞—è",
    # Fashion
    "–æ–¥–µ–∂–¥–∞",
    "–æ–±—É–≤—å",
    "–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã",
    "—Ä—é–∫–∑–∞–∫",
    "—Å—É–º–∫–∞",
    # Food
    "–∫–æ—Ñ–µ",
    "—á–∞–π",
    "—à–æ–∫–æ–ª–∞–¥",
    "–∫–æ–Ω—Ñ–µ—Ç—ã",
    "—Å–Ω–µ–∫–∏",
    # Sports
    "—Å–ø–æ—Ä—Ç",
    "—Ç—Ä–µ–Ω–∞–∂–µ—Ä",
    "–≥–∞–Ω—Ç–µ–ª–∏",
    "–∫–æ–≤—Ä–∏–∫",
]


def validate_yandex_market_url(url: str) -> bool:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç URL –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–∞"""
    try:
        parsed = urlparse(url)
        return (
            parsed.scheme in ("http", "https")
            and "market.yandex.ru" in parsed.netloc
            and (
                "/card/" in parsed.path
                or "/product/" in parsed.path
                or "/cc/" in parsed.path
            )
        )
    except Exception:
        return False


# –§—É–Ω–∫—Ü–∏—è fetch_with_retry —É–¥–∞–ª–µ–Ω–∞ - —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è HTTPClient


class AutoSearchService:
    def __init__(self, db, bot=None):
        self.db = db
        self.bot = bot
        self.http_client = HTTPClient()
        self.metrics = Metrics()

    async def search_products(
        self, query: str, max_results: int = 20, page: int = 1
    ) -> List[Dict]:
        """
        –ò—â–µ—Ç —Ç–æ–≤–∞—Ä—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –ø–∞—Ä—Å–∏–Ω–≥–∞.

        ENHANCEMENTS:
        - User-Agent rotation
        - Pagination support (&page=N)
        - JSON fallback parsing if HTML fails
        """
        with Timer("search_products", self.metrics):
            try:
                self.metrics.increment("search_products.attempts")

                # ENHANCEMENT: Add pagination for deeper results
                search_url = f"https://market.yandex.ru/search?text={quote(query)}"
                if page > 1:
                    search_url += f"&page={page}"

                # ENHANCEMENT: Random User-Agent rotation
                headers = {
                    "User-Agent": random.choice(USER_AGENTS),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                }

                html = await self.http_client.fetch_text(search_url, headers=headers)
                if not html:
                    self.metrics.record_error("search_products")
                    return []

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                try:
                    from lxml import html as lxml_html

                    soup = BeautifulSoup(html, "lxml")
                except ImportError:
                    soup = BeautifulSoup(html, "html.parser")

                products = []

                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
                product_links = soup.find_all("a", href=re.compile(r"/product/"))

                for link in product_links[:max_results]:
                    href = link.get("href", "")
                    if href.startswith("/"):
                        href = f"https://market.yandex.ru{href}"

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
                    if "/product/" in href and href not in [
                        p.get("url") for p in products
                    ]:
                        products.append(
                            {
                                "url": href,
                                "title": link.get_text(strip=True)[:100] or "–¢–æ–≤–∞—Ä",
                            }
                        )

                # ENHANCEMENT: JSON Fallback - parse <script> tags if HTML parsing found nothing
                if not products:
                    logger.info(
                        "HTML parsing found no products, trying JSON fallback..."
                    )
                    try:
                        # Find JSON data in <script> tags (common for SPAs)
                        script_tags = soup.find_all(
                            "script", type="application/ld+json"
                        )
                        for script in script_tags:
                            try:
                                data = json.loads(script.string)
                                # Look for product URLs in JSON
                                if isinstance(data, dict) and "url" in data:
                                    url = data.get("url", "")
                                    if "/product/" in url or "/card/" in url:
                                        products.append(
                                            {
                                                "url": (
                                                    f"https://market.yandex.ru{url}"
                                                    if url.startswith("/")
                                                    else url
                                                ),
                                                "title": data.get("name", "–¢–æ–≤–∞—Ä")[
                                                    :100
                                                ],
                                            }
                                        )
                            except json.JSONDecodeError:
                                continue
                    except Exception as e:
                        logger.debug(f"JSON fallback failed: {e}")

                logger.info(
                    f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)} (query='{query}', page={page})"
                )
                return products

            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤: {e}")
                return []

    async def search_by_category(
        self, category: str, max_results: int = 20
    ) -> List[Dict]:
        """–ò—â–µ—Ç —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_queries = {
            "–µ–¥–∞": ["—à–æ–∫–æ–ª–∞–¥", "–∫–æ–Ω—Ñ–µ—Ç—ã", "—Å–Ω–µ–∫–∏", "–∫–æ—Ñ–µ", "—á–∞–π"],
            "—Ç–µ—Ö–Ω–∏–∫–∞": ["–Ω–∞—É—à–Ω–∏–∫–∏", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "—Ç–µ–ª–µ—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫"],
            "–æ–¥–µ–∂–¥–∞": ["–æ–¥–µ–∂–¥–∞", "—Ä—É–±–∞—à–∫–∞", "–∫—É—Ä—Ç–∫–∞", "–æ–±—É–≤—å"],
            "–∏–≥—Ä—É—à–∫–∏": ["–∏–≥—Ä—É—à–∫–∏", "lego", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä"],
            "–∫–Ω–∏–≥–∏": ["–∫–Ω–∏–≥–∏", "—É—á–µ–±–Ω–∏–∫"],
            "–∫–æ—Å–º–µ—Ç–∏–∫–∞": ["–∫–æ—Å–º–µ—Ç–∏–∫–∞", "–∫—Ä–µ–º", "—à–∞–º–ø—É–Ω—å"],
        }

        queries = category_queries.get(category.lower(), [category])
        all_products = []

        for query in queries[:2]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –∑–∞–ø—Ä–æ—Å–∞
            products = await self.search_products(query, max_results // len(queries))
            all_products.extend(products)
            await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        return all_products[:max_results]

    async def get_products_from_main_page(self, max_results: int = 50) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–∞ https://market.yandex.ru/"""
        try:
            # –¢–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–∞
            main_page_url = "https://market.yandex.ru/"

            # –ó–∞–≥—Ä—É–∂–∞–µ–º cookies –µ—Å–ª–∏ –µ—Å—Ç—å
            cookies_file = os.path.join(os.path.dirname(__file__), "..", "cookies.json")
            cookies_header = ""
            if os.path.exists(cookies_file):
                try:
                    import json

                    with open(cookies_file, "r", encoding="utf-8") as f:
                        cookies_data = json.load(f)
                        if isinstance(cookies_data, list):
                            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É cookies –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                            cookies_parts = []
                            for cookie in cookies_data:
                                if (
                                    isinstance(cookie, dict)
                                    and "name" in cookie
                                    and "value" in cookie
                                ):
                                    cookies_parts.append(
                                        f"{cookie['name']}={cookie['value']}"
                                    )
                            cookies_header = "; ".join(cookies_parts)
                except Exception as e:
                    logger.debug(f"Failed to load cookies: {e}")

            headers = {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            }
            if cookies_header:
                headers["Cookie"] = cookies_header

            products = []
            seen_urls = set()

            # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_url = main_page_url
            try:
                with Timer("get_products_from_main_page.fetch", self.metrics):
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º fetch_text –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —á—Ç–µ–Ω–∏—è –∏ –∑–∞–∫—Ä—ã—Ç–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                    html = await self.http_client.fetch_text(page_url, headers=headers)
                    if not html:
                        self.metrics.record_error("get_products_from_main_page")
                        logger.error(
                            f"Failed to fetch main page after retries: {page_url[:100]}"
                        )
                        return []

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    try:
                        from lxml import html as lxml_html

                        soup = BeautifulSoup(html, "lxml")
                    except ImportError:
                        soup = BeautifulSoup(html, "html.parser")

                    category_products_count = 0

                    # –£–ü–†–û–©–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –ü—Ä–æ—Å—Ç–æ –∏—â–µ–º –æ–±—ã—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ /card/ –∏ /product/

                    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
                    page_text = html
                    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ /card/ –∏ /product/ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    card_matches = re.findall(
                        r'https?://market\.yandex\.ru/card/[^\s"\'<>\)]+', page_text
                    )
                    product_matches = re.findall(
                        r'https?://market\.yandex\.ru/product/[^\s"\'<>\)]+', page_text
                    )

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    for match_url in card_matches + product_matches:
                        if len(products) >= max_results:
                            break

                        # –û—á–∏—â–∞–µ–º URL –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                        match_url = (
                            match_url.split('"')[0]
                            .split("'")[0]
                            .split(">")[0]
                            .split("<")[0]
                            .split(")")[0]
                        )
                        match_url = match_url.split("#")[0]  # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è

                        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º URL
                        if not validate_yandex_market_url(match_url):
                            continue

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                        url_base = match_url.split("?")[0]
                        if url_base in seen_urls:
                            continue
                        seen_urls.add(url_base)

                        products.append(
                            {"url": match_url, "title": "–¢–æ–≤–∞—Ä —Å –Ø–Ω–¥–µ–∫—Å.–ú–∞—Ä–∫–µ—Ç–∞"}
                        )
                        category_products_count += 1

                    # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ HTML —Ç–µ–≥–∞—Ö <a>
                    all_links = soup.find_all("a", href=True)

                    for link in all_links:
                        if len(products) >= max_results:
                            break

                        href = link.get("href", "")

                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ —Ç–æ–≤–∞—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                        if not href or (
                            "/card/" not in href and "/product/" not in href
                        ):
                            continue

                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                        if href.startswith("/"):
                            href = f"https://market.yandex.ru{href}"

                        # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è –∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–∂–Ω—ã)
                        href_clean = href.split("#")[0]

                        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º URL
                        if not validate_yandex_market_url(href_clean):
                            continue

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —É–Ω–∏–∫–∞–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ (–ø–æ –±–∞–∑–æ–≤–æ–º—É URL –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
                        url_base = href_clean.split("?")[0]
                        if url_base in seen_urls:
                            continue
                        seen_urls.add(url_base)

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
                        title = link.get_text(strip=True)[:100] or "–¢–æ–≤–∞—Ä"

                        # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
                        if not title or title == "–¢–æ–≤–∞—Ä" or len(title) < 3:
                            parent = link.find_parent(
                                ["div", "article", "section", "li"]
                            )
                            if parent:
                                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                                for tag in ["h1", "h2", "h3", "h4", "span", "div"]:
                                    title_elem = parent.find(
                                        tag,
                                        class_=re.compile(
                                            r"title|name|product|card", re.I
                                        ),
                                    )
                                    if title_elem:
                                        title = title_elem.get_text(strip=True)[:100]
                                        if title and len(title) > 3:
                                            break

                                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Ä–æ–¥–∏—Ç–µ–ª—è
                                if not title or title == "–¢–æ–≤–∞—Ä":
                                    parent_text = parent.get_text(strip=True)[:100]
                                    if parent_text and len(parent_text) > 3:
                                        title = parent_text

                        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∞—Å—Ç—å URL
                        if not title or title == "–¢–æ–≤–∞—Ä":
                            url_parts = href_clean.split("/")
                            for part in reversed(url_parts):
                                if part and part not in [
                                    "card",
                                    "product",
                                    "market.yandex.ru",
                                ]:
                                    title = part.replace("-", " ").title()[:100]
                                    break

                        products.append(
                            {
                                "url": href_clean,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Å—ã–ª–∫—É —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                                "title": title or "–¢–æ–≤–∞—Ä",
                            }
                        )
                        category_products_count += 1

                    if category_products_count == 0:
                        logger.warning(
                            f"‚ö†Ô∏è –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ. HTML –¥–ª–∏–Ω–∞: {len(html)}. –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç."
                        )

                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(products)}")
                    return products

            except Exception as e:
                logger.exception(f"Error parsing main page: {e}")
                return []

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return []

    async def auto_add_products_from_main_page(self, max_add: int = 20):
        """
        SMART SCRAPING STRATEGY (Problem #7 + Enhancements):

        1. Try main page first
        2. If < 2 new products found ‚Üí IMMEDIATELY switch to keyword search
        3. Use random keywords + pagination for deeper results
        4. Uses fast O(1) duplicate checks via SQL index
        """
        try:
            added = 0
            skipped = 0

            # STEP 1: Try main page with expanded fetch
            logger.info("Step 1: Trying main page...")
            products = await self.get_products_from_main_page(max_add * 10)

            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–∫–∞—Ç—å, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
            # Process products from main page with fast O(1) duplicate checks
            for product in products:
                if added >= max_add:
                    break
                url = product["url"]

                # Blacklist filter check
                should_filter, filter_reason = should_filter_product(
                    product, reason_prefix=""
                )
                if should_filter:
                    skipped += 1
                    continue

                # Fast O(1) duplicate checks using SQL index
                if self.db.exists_url(url, check_normalized=True):
                    skipped += 1
                    continue

                # Extract product data for key generation
                title = product.get('title', '')
                vendor = product.get('vendor', '')
                offerid = product.get('offerid')

                product_key = make_product_key(title=title, vendor=vendor, offerid=offerid, url=url)

                # –ø—Ä–æ–≤–µ—Ä—è–µ–º: —É–∂–µ –≤ –æ—á–µ—Ä–µ–¥–∏ –∏–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω
                if self.db.queue_contains_product_key(product_key):
                    logger.info("Skip queue insert ‚Äî already in queue (key=%s) url=%s", product_key, url)
                    skipped += 1
                    continue
                else:
                    if self.db.has_recent_post(product_key, days=7):
                        logger.info("Skip queue insert ‚Äî similar post in last 7 days (key=%s) url=%s", product_key, url)
                        skipped += 1
                        continue
                    else:
                        # –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—Å—Ç–∞–≤–ª—è–µ–º; —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –≥–æ–Ω–∫–∏
                        self.db.add_queue_item_with_key(url=url, title=title, product_key=product_key)
                        added += 1
                        logger.info("Auto-search: added product to queue (key=%s) url=%s", product_key, url)

            logger.info(f"Step 1 result: added={added}, skipped={skipped}")

            # STEP 2: SMART FALLBACK - If < 2 new products ‚Üí switch to keyword search
            if added < 2:
                logger.info(
                    f"Step 2: Only {added} new products from main page. Switching to KEYWORD SEARCH..."
                )

                # Shuffle keywords for randomness
                keywords = random.sample(SEARCH_KEYWORDS, min(10, len(SEARCH_KEYWORDS)))

                for keyword in keywords:
                    if added >= max_add:
                        break

                    # ENHANCEMENT: Random pagination for deeper results (page 1 or 2)
                    page = random.randint(1, 2)
                    logger.info(f"Searching: '{keyword}' (page={page})")

                    try:
                        search_products = await self.search_products(
                            keyword, max_results=max_add * 3, page=page
                        )
                    except Exception as e:
                        logger.error(f"Search error for '{keyword}': {e}")
                        continue

                    # Process search results
                    for sp in search_products:
                        if added >= max_add:
                            break

                        url = sp["url"]

                        # Blacklist filter check
                        should_filter, filter_reason = should_filter_product(
                            sp, reason_prefix=""
                        )
                        if should_filter:
                            skipped += 1
                            continue

                        # Fast O(1) checks
                        if self.db.exists_url(url, check_normalized=True):
                            skipped += 1
                            continue

                        # Extract product data for key generation
                        title = product.get('title', '')
                        vendor = product.get('vendor', '')
                        offerid = product.get('offerid')

                        product_key = make_product_key(title=title, vendor=vendor, offerid=offerid, url=url)

                        # –ø—Ä–æ–≤–µ—Ä—è–µ–º: —É–∂–µ –≤ –æ—á–µ—Ä–µ–¥–∏ –∏–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω
                        if self.db.queue_contains_product_key(product_key):
                            logger.info("Skip queue insert ‚Äî already in queue (key=%s) url=%s", product_key, url)
                            skipped += 1
                            continue
                        else:
                            if self.db.has_recent_post(product_key, days=7):
                                logger.info("Skip queue insert ‚Äî similar post in last 7 days (key=%s) url=%s", product_key, url)
                                skipped += 1
                                continue
                            else:
                                # –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—Å—Ç–∞–≤–ª—è–µ–º; —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –≥–æ–Ω–∫–∏
                                self.db.add_queue_item_with_key(url=url, title=title, product_key=product_key)
                                added += 1
                                logger.info("Auto-search: added product to queue (key=%s) url=%s", product_key, url)
                        logger.info(f"‚úÖ Keyword '{keyword}': added {url[:100]}...")

                    await asyncio.sleep(0.5)  # Small delay between keyword searches

            logger.info(f"FINAL: added={added}, skipped={skipped}")
            return added

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤: {e}")
            return 0

    async def auto_add_products(
        self, query: str = None, category: str = None, max_add: int = 10
    ):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–≤–∞—Ä—ã –≤ –æ—á–µ—Ä–µ–¥—å"""
        try:
            if category:
                products = await self.search_by_category(category, max_add * 2)
            elif query:
                products = await self.search_products(query, max_add * 2)
            else:
                logger.warning("–ù–µ —É–∫–∞–∑–∞–Ω –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è")
                return 0

            added = 0
            skipped = 0

            for product in products:
                url = product["url"]

                # Blacklist filter check
                should_filter, filter_reason = should_filter_product(
                    product, reason_prefix=""
                )
                if should_filter:
                    skipped += 1
                    continue

                # Extract product data for key generation
                title = product.get('title', '')
                vendor = product.get('vendor', '')
                offerid = product.get('offerid')

                product_key = make_product_key(title=title, vendor=vendor, offerid=offerid, url=url)

                # –ø—Ä–æ–≤–µ—Ä—è–µ–º: —É–∂–µ –≤ –æ—á–µ—Ä–µ–¥–∏ –∏–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω
                if self.db.queue_contains_product_key(product_key):
                    logger.info("Skip queue insert ‚Äî already in queue (key=%s) url=%s", product_key, url)
                    skipped += 1
                    continue
                else:
                    if self.db.has_recent_post(product_key, days=7):
                        logger.info("Skip queue insert ‚Äî similar post in last 7 days (key=%s) url=%s", product_key, url)
                        skipped += 1
                        continue
                    else:
                        # –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—Å—Ç–∞–≤–ª—è–µ–º; —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –≥–æ–Ω–∫–∏
                        self.db.add_queue_item_with_key(url=url, title=title, product_key=product_key)
                        added += 1
                        logger.info("Auto-search: added product to queue (key=%s) url=%s", product_key, url)

                if added >= max_add:
                    break

            logger.info(f"–ê–≤—Ç–æ–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ: –¥–æ–±–∞–≤–ª–µ–Ω–æ {added}, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped}")
            return added

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {e}")
            return 0

    async def run_search_and_queue(self, global_settings, bot=None):
        """
        Run auto search and queue products according to settings.
        This method replaces the auto_search_worker function and is designed to be called by APScheduler.

        Args:
            global_settings: Global settings object with configuration
            bot: Bot instance for notifications (optional)
        """
        try:
            # Get settings from Pydantic config
            from config import settings

            AUTO_SEARCH_ENABLED = settings.AUTO_SEARCH_ENABLED
            AUTO_MAIN_PAGE_ENABLED = settings.AUTO_MAIN_PAGE_ENABLED
            # Use default threshold since it's not in Pydantic config
            AUTO_SEARCH_HOURS_THRESHOLD = 3

            if not AUTO_SEARCH_ENABLED and not AUTO_MAIN_PAGE_ENABLED:
                logger.debug("Auto search disabled")
                return

            # Check if enough time has passed since last post
            from datetime import datetime

            last_post_time = self.db.get_last_post_time()
            hours_since_last_post = 999  # Default large value

            if last_post_time:
                # Convert to datetime if needed
                if isinstance(last_post_time, str):
                    try:
                        last_post_time = datetime.fromisoformat(
                            last_post_time.replace("Z", "+00:00")
                        )
                    except ValueError:
                        try:
                            last_post_time = datetime.strptime(
                                last_post_time, "%Y-%m-%d %H:%M:%S"
                            )
                        except ValueError:
                            last_post_time = datetime.strptime(
                                last_post_time, "%Y-%m-%d %H:%M:%S.%f"
                            )

                time_since_last_post = datetime.now() - last_post_time
                hours_since_last_post = time_since_last_post.total_seconds() / 3600

                if hours_since_last_post < AUTO_SEARCH_HOURS_THRESHOLD:
                    logger.debug(
                        f"‚è≥ –° –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ—Å—Ç–∞ –ø—Ä–æ—à–ª–æ {hours_since_last_post:.1f} —á–∞—Å–æ–≤, –Ω—É–∂–Ω–æ {AUTO_SEARCH_HOURS_THRESHOLD} —á–∞—Å–æ–≤. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–ø–æ–∏—Å–∫."
                    )
                    return

            # Check if queue is empty
            queue_size = self.db.get_queue_size()
            if queue_size > 0:
                logger.debug(
                    f"‚è≥ –í –æ—á–µ—Ä–µ–¥–∏ —É–∂–µ –µ—Å—Ç—å {queue_size} —Ç–æ–≤–∞—Ä–æ–≤. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–ø–æ–∏—Å–∫."
                )
                return

            logger.info(
                f"üîç Auto search run (–ø—Ä–æ—à–ª–æ {hours_since_last_post:.1f} —á–∞—Å–æ–≤ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ—Å—Ç–∞)"
            )

            total_added = 0

            # Search products from main page if enabled
            if AUTO_MAIN_PAGE_ENABLED:
                logger.info("üîó –ü–æ–ª—É—á–∞—é —Ç–æ–≤–∞—Ä—ã —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (max 1)...")
                try:
                    added = await self.auto_add_products_from_main_page(max_add=1)
                    total_added += added
                    if added > 0:
                        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Ç–æ–≤–∞—Ä —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {added}")
                except Exception as e:
                    logger.exception(f"Error getting products from main page: {e}")

            if total_added > 0:
                logger.info(f"Auto search added {total_added} product")
                # Track last successful auto_search run
                self.db.set_setting(
                    "last_auto_search_run", datetime.utcnow().isoformat()
                )

                # Send notification if bot is provided
                if bot:
                    import config

                    if config.ADMIN_ID:
                        try:
                            await bot.send_message(
                                config.ADMIN_ID,
                                f"üîç –ê–≤—Ç–æ–ø–æ–∏—Å–∫: –¥–æ–±–∞–≤–ª–µ–Ω {total_added} —Ç–æ–≤–∞—Ä –≤ –æ—á–µ—Ä–µ–¥—å",
                            )
                        except (Exception, asyncio.TimeoutError) as e:
                            logger.debug(f"Failed to send notification: {e}")

        except Exception as e:
            logger.exception("run_search_and_queue error: %s", e)
